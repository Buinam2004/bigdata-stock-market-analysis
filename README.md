# Bigdata Stock Market Analysis

Real-time stock data pipeline with live technical analysis dashboard using Kafka, Spark Streaming, Apache Iceberg, MinIO, and Streamlit.

## âš¡ Quick Start

**Option 1: Automated Start (Windows)**

```bash
# One-click startup (opens 5 terminal windows)
start_pipeline.bat
```

**Option 2: Manual Start**

```bash
# 1. Start infrastructure
docker compose up -d

# 2. Run in separate terminals:
cd raw_data_bronze && python spark_to_iceberg_bronze.py
cd processed_data_silver && python bronze_to_silver.py
cd processed_data_gold && python silver_to_gold_streaming.py
cd processed_data_gold && streamlit run dashboard.py  # Opens at http://localhost:8501
```

**Check Pipeline Status**

```bash
python check_pipeline_status.py
```

ğŸ“– **Detailed Guide**: See [QUICKSTART_DASHBOARD.md](QUICKSTART_DASHBOARD.md)

---

## ğŸ¯ What You Get

- **ğŸ“Š Live Dashboard** at http://localhost:8501 with:

  - Real-time candlestick charts
  - Technical indicators (SMA, RSI, MACD, Bollinger Bands)
  - Trading signals (BUY/SELL/HOLD)
  - Sector performance analysis
  - Auto-refresh every 5-60 seconds

- **ğŸ”„ Real-time Streaming Pipeline**:

  - Bronze Layer: Raw data ingestion
  - Silver Layer: Data quality & validation
  - Gold Layer: Technical indicators & signals

- **ğŸ“ˆ 50+ Stock Symbols** tracked in real-time
- **âš¡ End-to-end latency**: 30-90 seconds
- **ğŸ¨ Beautiful UI** with Streamlit and Plotly

ğŸ“š **Full Feature List**: See [FEATURES.md](FEATURES.md)

---

## Architecture

```
Yahoo Finance â†’ Kafka â†’ Bronze Layer â†’ Silver Layer â†’ Gold Layer â†’ Dashboard
     API              (Raw Data)    (Cleansed)    (Analytics)   (Streamlit)
                          â†“              â†“             â†“
                      Iceberg        Iceberg       Iceberg
                       MinIO          MinIO         MinIO
```

### Data Layers (Medallion Architecture)

- **Bronze Layer** ğŸ¥‰: Raw, unprocessed data from sources (append-only)
- **Silver Layer** ğŸ¥ˆ: Cleansed, validated, deduplicated data (quality-checked)
- **Gold Layer** ğŸ¥‡: Aggregated analytics with technical indicators (business-ready)
- **Dashboard** ğŸ“Š: Real-time Streamlit UI with live charts and signals

## Tech Stack

- **Kafka (KRaft mode)**: Message streaming (no ZooKeeper)
- **Apache Spark**: Stream processing & technical indicators
- **Apache Iceberg**: Data lakehouse table format with time-travel
- **MinIO**: S3-compatible object storage
- **Streamlit**: Real-time analytics dashboard
- **Plotly**: Interactive charts and visualizations
- **Python**: Data ingestion, processing, and analytics

## Quick Start

### 1. Start infrastructure (Kafka + MinIO) with Docker Compose

```bash
docker compose up -d
```

This starts only the infrastructure:

- **MinIO** on ports 9000 (API) and 9001 (Console)
- **Kafka** on port 9092 (KRaft mode, no ZooKeeper)
- **Bucket init** via `minio-init`

### 2. Access Services

- **MinIO Console**: http://localhost:9001 (minioadmin/minioadmin)
- **Kafka**: localhost:9092

### 3. Create Kafka Topic

```bash
docker exec kafka kafka-topics --create \
  --if-not-exists \
  --bootstrap-server localhost:9092 \
  --topic stock_ticks \
  --partitions 1 \
  --replication-factor 1
```

### 4. Verify Data in MinIO

1. Open MinIO Console at http://localhost:9001
2. Login with `minioadmin` / `minioadmin`
3. Check the `warehouse` bucket for Iceberg data

### 5. Run the pipeline locally (no containers for code)

1. Copy env vars and install deps

```bash
cp .env.example .env
cd raw_data_bronze
pip install -r requirements.txt
```

2. Export env vars

```bash
# PowerShell
$env:KAFKA_BOOTSTRAP_SERVERS="localhost:9092"
$env:KAFKA_TOPIC="stock_ticks"
$env:MINIO_ENDPOINT="http://localhost:9000"
$env:MINIO_ACCESS_KEY="minioadmin"
$env:MINIO_SECRET_KEY="minioadmin"
$env:ICEBERG_WAREHOUSE="s3a://warehouse"

# Optional: checkpoint dir
$env:CHECKPOINT_LOCATION="./checkpoints/iceberg_bronze"
```

3. Run producer locally

```bash
cd raw_data_bronze
python yahoo_to_kafka.py
```

4. Run Spark streaming locally (new terminal)

```bash
cd raw_data_bronze
spark-submit \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367 \
  --conf spark.hadoop.fs.s3a.endpoint=http://localhost:9000 \
  --conf spark.hadoop.fs.s3a.access.key=minioadmin \
  --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  spark_to_iceberg_bronze.py
```

5. Query Iceberg locally (new terminal)

```bash
cd raw_data_bronze
spark-submit \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367 \
  query_iceberg_bronze.py
```

## Project Structure

```
.
â”œâ”€â”€ docker-compose.yml              # Service orchestration
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ producer/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ spark/
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ raw_data_bronze/
â”‚   â”œâ”€â”€ yahoo_to_kafka.py          # Producer: Yahoo Finance â†’ Kafka
â”‚   â”œâ”€â”€ kafka_to_spark.py          # Test: Kafka â†’ Spark console
â”‚   â”œâ”€â”€ spark_to_iceberg_bronze.py # Main: Spark â†’ Iceberg â†’ MinIO
â”‚   â”œâ”€â”€ query_iceberg_bronze.py    # Query Iceberg tables
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ processed_data_silver/
â”‚   â”œâ”€â”€ bronze_to_silver.py        # Silver: Data quality & cleansing
â”‚   â”œâ”€â”€ query_iceberg_silver.py    # Query Silver layer
â”œâ”€â”€ processed_data_gold/
â”‚   â”œâ”€â”€ silver_to_gold_streaming.py # Gold: Technical indicators & signals
â”‚   â”œâ”€â”€ dashboard.py               # Streamlit real-time dashboard
â”‚   â”œâ”€â”€ query_iceberg_gold.py      # Query Gold layer
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md                  # Gold layer documentation
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md                  # Silver layer documentation
â””â”€â”€ .env.example                    # Environment variables template
```

## Development

Develop and run everything locally; Docker is used only for Kafka/MinIO. Follow the Quick Start section to set env vars, install deps, and run `yahoo_to_kafka.py` and `spark_to_iceberg_bronze.py` directly from your machine.

## Key Changes from Original Setup

âœ… **Removed ZooKeeper** - Kafka now runs in KRaft mode  
âœ… **Removed Hadoop dependency** - Using MinIO S3 instead of local HDFS  
âœ… **Docker for infra only** - Kafka/MinIO in containers; code runs locally  
âœ… **Spark on host** - Use your local Spark install (3.5.7)  
âœ… **S3-compatible storage** - Iceberg data stored in MinIO  
âœ… **Environment variables** - Easy configuration for Docker and local dev

## Troubleshooting

### View logs

```bash
# Infrastructure services
docker compose logs -f

# Kafka only
docker compose logs -f kafka
```

### Restart services

```bash
docker compose restart kafka
docker compose restart minio
```

### Clean up and restart

```bash
docker compose down -v
docker compose up -d
```

### Initialize MinIO bucket manually

If the warehouse bucket doesn't exist:

```bash
docker run --rm --network bigdata-stock-market-analysis_default \
  --entrypoint sh minio/mc -c "\
  mc alias set myminio http://minio:9000 minioadmin minioadmin && \
  mc mb myminio/warehouse --ignore-existing"
```

## Data Flow

### Complete Real-time Streaming Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Yahoo Finance  â”‚
â”‚      API        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ 30s interval
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Kafka       â”‚
â”‚  stock_ticks    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ streaming
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Bronze Layer (Raw Data)                â”‚
â”‚  â€¢ Append-only ingestion                            â”‚
â”‚  â€¢ No transformations                               â”‚
â”‚  â€¢ Time-travel enabled                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ streaming
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Silver Layer (Data Quality)               â”‚
â”‚  â€¢ Price & volume validation                        â”‚
â”‚  â€¢ Deduplication                                    â”‚
â”‚  â€¢ Anomaly detection                                â”‚
â”‚  â€¢ Missing value imputation                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ streaming (15s batches)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Gold Layer (Technical Analytics)             â”‚
â”‚  â€¢ Moving Averages (SMA 5, 20, 50)                  â”‚
â”‚  â€¢ RSI (Relative Strength Index)                    â”‚
â”‚  â€¢ MACD & Signal Line                               â”‚
â”‚  â€¢ Bollinger Bands                                  â”‚
â”‚  â€¢ Trading Signals (Buy/Sell/Hold)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ real-time query
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Streamlit Dashboard (Live UI)               â”‚
â”‚  â€¢ Real-time candlestick charts                     â”‚
â”‚  â€¢ Technical indicator overlays                     â”‚
â”‚  â€¢ Trading signals table                            â”‚
â”‚  â€¢ Sector performance                               â”‚
â”‚  â€¢ Auto-refresh (5-60s)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Running the Complete Pipeline

**Terminal 1: Bronze Layer (Raw Ingestion)**

```bash
cd raw_data_bronze
python spark_to_iceberg_bronze.py
```

**Terminal 2: Silver Layer (Data Quality)**

```bash
cd processed_data_silver
python bronze_to_silver.py
```

**Terminal 3: Gold Layer (Technical Indicators)** ğŸ†•

```bash
cd processed_data_gold
python silver_to_gold_streaming.py
```

**Terminal 4: Real-time Dashboard** ğŸ†•

```bash
cd processed_data_gold
streamlit run dashboard.py
# Opens at http://localhost:8501
```

**Terminal 5: Producer (if not running)**

```bash
cd raw_data_bronze
python yahoo_to_kafka.py
```

### Dashboard Features ğŸ“Š

The Streamlit dashboard provides:

- **ğŸ“ˆ Live Market Overview**: Total stocks, Buy/Sell/Hold signals, Average RSI
- **ğŸ¯ Trading Signals Table**: Color-coded recommendations with price changes
- **ğŸ“Š Technical Charts**: Interactive candlestick charts with:
  - Bollinger Bands overlay
  - Moving Averages (SMA 5, 20, 50)
  - Volume bars
  - RSI indicator with oversold/overbought levels
- **ğŸ“‰ MACD Charts**: MACD line, signal line, and histogram
- **ğŸ¢ Sector Performance**: Aggregated metrics by sector
- **ğŸ”„ Auto-refresh**: Configurable refresh interval (5-60 seconds)
- **ğŸ›ï¸ Filters**: Select specific symbols and sectors

See detailed documentation:

- [Silver Layer README](processed_data_silver/README.md)
- [Gold Layer README](processed_data_gold/README.md)

## Query Data

**Bronze Layer**:

```bash
cd raw_data_bronze
python query_iceberg_bronze.py
```

**Silver Layer**:

```bash
cd processed_data_silver
python query_iceberg_silver.py
```

**Gold Layer** ğŸ†•:

```bash
cd processed_data_gold
python query_iceberg_gold.py
```

## Stop Services

```bash
docker compose down

# To also remove volumes (data will be lost)
docker compose down -v
```

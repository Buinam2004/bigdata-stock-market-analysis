"""
Export Iceberg Bronze data to Parquet format for easy sharing
ÄÆ¡n giáº£n nháº¥t: Export ra Parquet rá»“i upload lÃªn Google Drive/Dropbox
"""

from pyspark.sql import SparkSession
import os

print("="*60)
print("EXPORT ICEBERG TO PARQUET FOR SHARING")
print("="*60)

# Initialize Spark with Iceberg
spark = SparkSession.builder \
    .appName("Export Iceberg to Parquet") \
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0") \
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.local.type", "hadoop") \
    .config("spark.sql.catalog.local.warehouse", "D:/Bigdata/iceberg-warehouse") \
    .getOrCreate()

print("\nâœ… SparkSession initialized!")

# Read from Iceberg
print("\nğŸ“– Reading from Iceberg table: stock_db.stock_bronze")
df = spark.table("local.stock_db.stock_bronze")

# Show sample data
print("\nğŸ“Š Sample data:")
df.show(5, truncate=False)

# Count records
record_count = df.count()
print(f"\nğŸ“ˆ Total records: {record_count}")

# Export path
export_path = "D:/Bigdata/export/stock_bronze_parquet"

# Create export directory if not exists
os.makedirs("D:/Bigdata/export", exist_ok=True)

# Export to Parquet
print(f"\nğŸ’¾ Exporting to Parquet: {export_path}")
df.write.mode("overwrite").parquet(export_path)

print("\n" + "="*60)
print("âœ… EXPORT COMPLETED!")
print("="*60)
print(f"\nğŸ“ Exported to: {export_path}")
print(f"ğŸ“Š Total records: {record_count}")

# Check file size
import glob
parquet_files = glob.glob(f"{export_path}/*.parquet")
total_size = sum(os.path.getsize(f) for f in parquet_files)
print(f"ğŸ’¾ Total size: {total_size / (1024*1024):.2f} MB")

print("\n" + "="*60)
print("ğŸ“¤ NEXT STEPS TO SHARE:")
print("="*60)
print("1. NÃ©n folder: D:/Bigdata/export/stock_bronze_parquet")
print("2. Upload lÃªn Google Drive hoáº·c Dropbox")
print("3. Share link download vá»›i ngÆ°á»i khÃ¡c")
print("\n4. NgÆ°á»i khÃ¡c Ä‘á»c báº±ng:")
print("   df = spark.read.parquet('path/to/stock_bronze_parquet')")
print("   df.show()")
print("="*60)

spark.stop()

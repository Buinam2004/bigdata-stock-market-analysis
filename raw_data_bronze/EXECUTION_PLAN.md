# HÆ¯á»šNG DáºªN CHáº Y PIPELINE: YAHOO FINANCE â†’ KAFKA â†’ SPARK â†’ ICEBERG BRONZE (SPARK CHáº Y LOCAL)

> **Pipeline**: Thu tháº­p dá»¯ liá»‡u cá»• phiáº¿u real-time tá»« Yahoo Finance â†’ Kafka â†’ Spark Streaming â†’ Iceberg Bronze Layer
>
> Háº¡ táº§ng (Kafka, MinIO) cháº¡y báº±ng Docker. Code (producer + Spark) cháº¡y trá»±c tiáº¿p trÃªn mÃ¡y local.

---

## PREREQUISITES - CHUáº¨N Bá»Š MÃ”I TRÆ¯á»œNG

### YÃªu cáº§u há»‡ thá»‘ng

| Component       | Version           | Link                                                                        |
| --------------- | ----------------- | --------------------------------------------------------------------------- |
| Java            | 11 hoáº·c 17        | [Oracle JDK](https://www.oracle.com/java/technologies/downloads/)           |
| Python          | 3.8+ nhá» hÆ¡n 3.13 | [Python](https://www.python.org/downloads/)                                 |
| Apache Spark    | 3.5.7             | [Spark](https://spark.apache.org/downloads.html)                            |
| Docker Desktop  | Latest            | [Docker](https://www.docker.com/products/docker-desktop/)                   |
| Hadoop winutils | 3.3.6             | [GitHub](https://github.com/cdarlint/winutils/tree/master/hadoop-3.3.6/bin) |

### ğŸ”§ CÃ i Ä‘áº·t nhanh

**1. Java & Spark**

```bash
# Kiá»ƒm tra Java
java -version  # Cáº§n: 11.x hoáº·c 17.x

# Set biáº¿n mÃ´i trÆ°á»ng (Windows)
JAVA_HOME=C:\Program Files\Java\jdk-17
SPARK_HOME=C:\spark-3.5.7-bin-hadoop3
HADOOP_HOME=%SPARK_HOME%\hadoop
Path=%JAVA_HOME%\bin;%SPARK_HOME%\bin;%Path%

# Download hadoop.dll + winutils.exe â†’ C:\spark-3.5.7-bin-hadoop3\hadoop\bin\
```

**2. Python packages**

```bash
pip install yfinance kafka-python pyspark
python --version  # Cáº§n: 3.8+
```

**3. Docker**

```bash
docker --version
docker-compose --version
```

**4. Visual C++ Runtime (náº¿u lá»—i MSVCR100.dll)**

```bash
winget install Microsoft.VCRedist.2010.x64
```

---

## SETUP DOCKER SERVICES

### 1. Táº¡o file docker-compose.yml

**File Ä‘Ã£ cÃ³ táº¡i:** `d:/bigdata-stock-market-analysis/docker-compose.yml`

**Ná»™i dung:**

```yaml
version: "3"
services:
  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - "9092:9092"
    environment:
      # KRaft mode (no ZooKeeper)
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_KRAFT_CLUSTER_ID: "q1Sh-9_ISia_zwGINzRvyQ"
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:29093"

      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: "CONTROLLER:PLAINTEXT,INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT"
      KAFKA_LISTENERS: "INTERNAL://kafka:29092,EXTERNAL://0.0.0.0:9092,CONTROLLER://kafka:29093"
      KAFKA_ADVERTISED_LISTENERS: "INTERNAL://kafka:29092,EXTERNAL://localhost:9092"
      KAFKA_INTER_BROKER_LISTENER_NAME: "INTERNAL"
      KAFKA_CONTROLLER_LISTENER_NAMES: "CONTROLLER"

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1

  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
```

### 2. Khá»Ÿi Ä‘á»™ng Docker services

```bash
cd d:/bigdata-stock-market-analysis
docker compose up -d
```

### 3. Verify services Ä‘ang cháº¡y

```bash
docker ps
```

**Káº¿t quáº£ mong Ä‘á»£i:**

```
NAMES       STATUS          PORTS
kafka       Up X minutes    0.0.0.0:9092->9092/tcp
minio       Up X minutes    0.0.0.0:9000-9001->9000-9001/tcp
```

### 4. Táº¡o Kafka topic (chá»‰ láº§n Ä‘áº§u)

```bash
docker exec kafka kafka-topics --create \
  --bootstrap-server localhost:9092 \
  --topic stock_ticks \
  --partitions 1 \
  --replication-factor 1
```

**Verify topic:**

```bash
docker exec kafka kafka-topics --list --bootstrap-server localhost:9092
# Pháº£i tháº¥y: stock_ticks
```

---

## PHASE 2: YAHOO FINANCE â†’ KAFKA PRODUCER

### Má»¥c tiÃªu

Thu tháº­p dá»¯ liá»‡u cá»• phiáº¿u tá»« Yahoo Finance  
Gá»­i vÃ o Kafka topic `stock_ticks` má»—i 30 giÃ¢y

### TERMINAL 1: Cháº¡y Producer

**Má»Ÿ Terminal 1:**

```bash
cd d:/bigdata-stock-market-analysis/raw_data_bronze
python yahoo_to_kafka.py
```

**Expected Output:**

```
============================================================
START STREAMING STOCK DATA TO KAFKA
Topic: stock_ticks
Symbols: 29 stocks (AAPL, MSFT, GOOGL, META, TSLA...)
Interval: 30 seconds
============================================================

[2026-01-10 14:30:00] Fetching data...
  [OK] AAPL   | Technology | $259.35
  [OK] MSFT   | Technology | $479.20
  [OK] GOOGL  | Technology | $328.48
  ...
Successfully sent 29 stocks to Kafka!

[2026-01-10 14:30:30] Fetching data...
  [OK] AAPL   | Technology | $259.42
  ...
```

**Äá»‚ TERMINAL NÃ€Y CHáº Y LIÃŠN Tá»¤C!**

---

### TERMINAL 2: Test Spark Streaming

**Má»Ÿ Terminal má»›i (Terminal 2):**

```bash
cd d:/bigdata-stock-market-analysis/raw_data_bronze
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.7 \
  kafka_to_spark.py
```

** Expected Output (rÃºt gá»n):**

```
PHASE 3: Kafka -> Spark Structured Streaming
SparkSession initialized!

Parsed DataFrame Schema:
root
 |-- symbol: string
 |-- sector: string
 |-- close: double
 |-- volume: long
 |-- event_time: string

Batch: 0
+------+----------+--------+-------+
|symbol|sector    |close   |volume |
+------+----------+--------+-------+
|AAPL  |Technology|259.35  |318089 |
|MSFT  |Technology|479.20  |184154 |
+------+----------+--------+-------+
```

**Náº¿u tháº¥y data streaming â†’ PHASE 3 PASSED!**

- Phase 3 CHá»ˆ Äá»‚ TEST - CÃ³ thá»ƒ:\*\*

* **Option 1**: Nháº¥n Ctrl+C dá»«ng, chuyá»ƒn sang Phase 4 (khuyáº¿n nghá»‹)
* **Option 2**: Äá»ƒ cháº¡y tiáº¿p (monitor data), cháº¡y Phase 4 á»Ÿ Terminal 3
* **Option 3**: Bá» qua Phase 3, cháº¡y tháº³ng Phase 4 luÃ´n

---

## PHASE 4: SPARK â†’ ICEBERG BRONZE (NHIá»†M Vá»¤ CHÃNH)

```
============================================================
PHASE 3: Kafka -> Spark Structured Streaming
============================================================
SparkSession initialized!

Reading streaming data from Kafka topic: stock_ticks

Parsed DataFrame Schema:
root
 |-- symbol: string (nullable = true)
 |-- sector: string (nullable = true)
 |-- open: double (nullable = true)
 |-- high: double (nullable = true)
 |-- low: double (nullable = true)
 |-- close: double (nullable = true)
 |-- volume: long (nullable = true)
 |-- event_time: string (nullable = true)
 |-- source: string (nullable = true)

Starting streaming query to console...
Press Ctrl+C to stop.

-------------------------------------------
Batch: 0
-------------------------------------------
+------+----------+------------------+-------+-------------------------+
|symbol|sector    |close             |volume |event_time               |
+------+----------+------------------+-------+-------------------------+
|AAPL  |Technology|259.3500061035156 |318089 |2026-01-10T15:59:00-05:00|
|MSFT  |Technology|479.20001220703125|184154 |2026-01-10T15:59:00-05:00|
|GOOGL |Technology|328.4800109863281 |219387 |2026-01-10T15:59:00-05:00|
|META  |Technology|653.0399780273438 |114756 |2026-01-10T15:59:00-05:00|
|NVDA  |Technology|184.85000610351562|1018063|2026-01-10T15:59:00-05:00|
...
+------+----------+------------------+-------+-------------------------+
```

### TiÃªu chÃ­ Pass Phase 3

- [x] Spark job cháº¡y khÃ´ng lá»—i
- [x] KhÃ´ng lá»—i schema (tháº¥y root schema vá»›i 9 columns)
- [x] DataFrame hiá»ƒn thá»‹ Ä‘Ãºng dá»¯ liá»‡u (symbol, sector, close, volume, event_time)

**Náº¿u tháº¥y dá»¯ liá»‡u streaming â†’ PHASE 3 PASSED!**

**Nháº¥n Ctrl+C Ä‘á»ƒ dá»«ng Terminal 2, chuáº©n bá»‹ Phase 4**

---

## PHASE 4: SPARK â†’ ICEBERG BRONZE (NHIá»†M Vá»¤ CHÃNH)

### Má»¥c tiÃªu

Ghi dá»¯ liá»‡u streaming vÃ o Iceberg  
Táº¡o báº£ng stock_bronze vá»›i cá»™t ingest_time vÃ  source

### TERMINAL 2 (hoáº·c Terminal 3 náº¿u giá»¯ Phase 3)

**Cháº¡y Phase 4:**

```bash
cd d:/bigdata-stock-market-analysis/raw_data_bronze
spark-submit \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.7,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367 \
  --conf spark.hadoop.fs.s3a.endpoint=http://localhost:9000 \
  --conf spark.hadoop.fs.s3a.access.key=minioadmin \
  --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  spark_to_iceberg_bronze.py
```

> **LÆ°u Ã½:** Phase 3 vÃ  Phase 4 CÃ“ THá»‚ cháº¡y Ä‘á»“ng thá»i (cÃ¹ng Ä‘á»c tá»« Kafka topic `stock_ticks`)

** Expected Output (rÃºt gá»n):**

```
PHASE 4: Spark Streaming -> Iceberg Bronze
SparkSession with Iceberg initialized!
Warehouse: D:/Bigdata/iceberg-warehouse

Table stock_bronze created successfully!

Streaming to Iceberg Bronze STARTED!
Data is being written to: iceberg-warehouse/stock_db/stock_bronze
```

**Äá»‚ TERMINAL NÃ€Y CHáº Y LIÃŠN Tá»¤C!**

### Äá»¢I 2-3 PHÃšT

Dá»¯ liá»‡u sáº½ tÃ­ch lÅ©y ~60-90 records (29 stocks x 3 batches)

---

## PHASE 5: VERIFY ICEBERG (TÃ™Y CHá»ŒN)

````

**Äá»‚ TERMINAL NÃ€Y CHáº Y, KHÃ”NG Táº®T!**

---

### Äá»¢I 2-3 PHÃšT

**Äá»ƒ pipeline tÃ­ch lÅ©y dá»¯ liá»‡u:**

- Producer (Terminal 1) gá»­i data má»—i 30 giÃ¢y
- Spark (Terminal 2) xá»­ lÃ½ theo micro-batch
- Iceberg ghi vÃ o warehouse

**Sau 2-3 phÃºt, sáº½ cÃ³ ~60-90 records trong báº£ng**

---

## PHASE 5: VERIFY ICEBERG BRONZE (XÃC MINH Dá»® LIá»†U)

### Má»¥c tiÃªu

Kiá»ƒm tra dá»¯ liá»‡u Ä‘Ã£ ghi vÃ o Iceberg
XÃ¡c minh schema, partitions, vÃ  data quality

### TERMINAL 3: Má»Ÿ terminal má»›i

**Má» TERMINAL Má»šI (thá»© 3) Ä‘á»ƒ khÃ´ng lÃ m giÃ¡n Ä‘oáº¡n Terminal 1 & 2**

```bash
cd d:/bigdata-stock-market-analysis/raw_data_bronze
spark-submit \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.367 \
  --conf spark.hadoop.fs.s3a.endpoint=http://localhost:9000 \
  --conf spark.hadoop.fs.s3a.access.key=minioadmin \
  --conf spark.hadoop.fs.s3a.secret.key=minioadmin \
  --conf spark.hadoop.fs.s3a.path.style.access=true \
  --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
  --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false \
  query_iceberg_bronze.py
````

---

## Tá»”NG Káº¾T PIPELINE END-TO-END

```
Yahoo Finance â†’ Kafka â†’ Spark Streaming â†’ Iceberg Bronze
```

### Cáº¥u trÃºc project

```
D:/Bigdata/
â”œâ”€â”€ docker-compose.yml              # Kafka (KRaft, no ZooKeeper) + MinIO
â”œâ”€â”€ yahoo_to_kafka.py              # Phase 2: Producer
â”œâ”€â”€ kafka_to_spark.py              # Phase 3: Test streaming
â”œâ”€â”€ spark_to_iceberg_bronze.py     # Phase 4: Ghi vÃ o Iceberg
â”œâ”€â”€ query_iceberg_bronze.py        # Phase 5: Verify (optional)
â”œâ”€â”€ iceberg-warehouse/             # Iceberg data lake
â”‚   â””â”€â”€ stock_db/
â”‚       â””â”€â”€ stock_bronze/           # Báº£ng Bronze
â””â”€â”€ checkpoints/                   # Spark checkpoints
```

### Dá»«ng pipeline

```bash
# Terminal 1: Ctrl+C (dá»«ng Producer)
# Terminal 2: Ctrl+C (dá»«ng Spark streaming)

# Dá»«ng Docker
docker-compose down
```

---

## TROUBLESHOOTING (Lá»–I THÆ¯á»œNG Gáº¶P)

### 1. Kafka connection refused

**Triá»‡u chá»©ng:** `Connection refused to localhost:9092`

```bash
docker ps                  # Kiá»ƒm tra Kafka cháº¡y chÆ°a
docker-compose up -d       # Khá»Ÿi Ä‘á»™ng láº¡i
docker logs kafka          # Xem logs
```

### 2. Iceberg ClassNotFoundException

**Triá»‡u chá»©ng:** `java.lang.ClassNotFoundException: org.apache.iceberg.spark.SparkCatalog`

```bash
# Äáº£m báº£o cÃ³ --packages khi cháº¡y spark-submit
spark-submit \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0 \
  spark_to_iceberg_bronze.py
```

### 3. Windows Hadoop DLL error (exitCode=-1073741515)

**Triá»‡u chá»©ng:** `Py4JJavaError: exitCode=-1073741515`

```bash
# Download tá»«: https://github.com/cdarlint/winutils/tree/master/hadoop-3.3.6/bin
# Copy hadoop.dll vÃ o: C:\spark\hadoop\bin\
# CÃ i Visual C++ Runtime:
winget install Microsoft.VCRedist.2010.x64
```

### 4. Query khÃ´ng tráº£ vá» dá»¯ liá»‡u

**Triá»‡u chá»©ng:** `Query returns: 0 rows`

```bash
# Kiá»ƒm tra Producer Terminal 1 cÃ³ Ä‘ang gá»­i data
# Kiá»ƒm tra Spark Terminal 2 cÃ³ Ä‘ang cháº¡y
# Äá»£i thÃªm 1-2 phÃºt cho data Ä‘Æ°á»£c commit

# Kiá»ƒm tra thÆ° má»¥c Iceberg:
ls D:/Bigdata/iceberg-warehouse/stock_db/stock_bronze/data/
```

---

###Schema cá»§a bronze data

```bash
bronze_schema = StructType([
    # ThÃ´ng tin cá»• phiáº¿u
    StructField("symbol", StringType(), False),           # MÃ£ cá»• phiáº¿u (VD: AAPL, GOOGL)

    # GiÃ¡ cá»• phiáº¿u
    StructField("open", DoubleType(), True),              # GiÃ¡ má»Ÿ cá»­a
    StructField("high", DoubleType(), True),              # GiÃ¡ cao nháº¥t
    StructField("low", DoubleType(), True),               # GiÃ¡ tháº¥p nháº¥t
    StructField("close", DoubleType(), True),             # GiÃ¡ Ä‘Ã³ng cá»­a
    StructField("adj_close", DoubleType(), True),         # GiÃ¡ Ä‘Ã³ng cá»­a Ä‘iá»u chá»‰nh

    # Khá»‘i lÆ°á»£ng giao dá»‹ch
    StructField("volume", LongType(), True),              # Khá»‘i lÆ°á»£ng giao dá»‹ch

    # Thá»i gian
    StructField("timestamp", TimestampType(), False),     # Thá»i Ä‘iá»ƒm láº¥y dá»¯ liá»‡u
    StructField("date", StringType(), True),              # NgÃ y giao dá»‹ch (YYYY-MM-DD)

    # Metadata
    StructField("ingestion_time", TimestampType(), False) # Thá»i gian nháº­p vÃ o há»‡ thá»‘ng
])

```

_Last updated: 2024-01-15_
_Version: 1.0 - Complete End-to-End Pipeline_
